[aws]
# region is the region for the controlling bucket
# note that ideally, all buckets & instances should be
# in the same region otherwise there may be additional
# charges from AWS
region = us-west-2
############
# aws s3
############
s3-bucket = vccri.ho-lab.software-testing
# results - prefix for output copied to s3 (e.g. results = bob will add /bob/)
s3-results-dir = rescue-reads-results/mictro
############
# Instances
############
instance-tag = mictro-rescue-reads-auto
availability-zone = us-west-2c
# use spot instances - set to 1
use-spot = 1
spot-price = 5
# see constants.py file for list of allowable instance types
# At this stage not allowing EBS types
instance-type = c4.4xlarge
count = 1
# for ebs only instance types (ignored if not ebs only)
# ebs-only-volume-size - gigabytes
ebs-only-volume-size = 100
# ebs-only-volume-type: 'standard'|'io1'|'gp2'|'sc1'|'st1'
ebs-only-volume-type = gp2
#user-data = system/user-data-2x-ssd.sh
#user-data-checker = system/user-data-checker-2x-ssd.sh
# use -1x ssd for c4 types - EBS backed using gp2 ssd
user-data = system/user-data-1x-ssd.sh
user-data-checker = system/user-data-checker-1x-ssd.sh
#user-data = dev/user-data.sh
#user-data-checker = dev/user-data-checker.sh
dry-run = False
security-key = bioec2
image-id = ami-1307fb73
# sg name required for on-demand instances
security-groups = bioec2
# sg id required for spot instances
security-group-id = sg-8112f8e5
profile-name = EMR_EC2_DefaultRole
arn-id = arn:aws:iam::704266947548:instance-profile/EMR_EC2_DefaultRole
[rescue]
local-program-dir = /home/mike/rescue/rescue_unmapped_reads
# arguments for rescue reads - excluding the fastq input file(s)
# - also excluding output dir
# output dir will be <output-dir>/<run id>/<fastq id>
# - where <run id> is the uniq identifier for this aws run
# - <fastq id> is the line number in the manifest file that refers to the 
# fastq file(s) being used for this execution of the "pipeline"
# <output-dir> must be a pre-existing directory (that is created at instance
# startup)
#instance-output-dir = /mnt2/output
#command-arguments  =  -G /mnt2/ref/genome_ref/GRCm38.primary_assembly.genome.fa -g /mnt2/ref/star_ref -t 30 -at STAR
instance-output-dir = /mnt1/output
command-arguments  =  -G /mnt1/ref/genome_ref/GRCm38.primary_assembly.genome.fa -g /mnt1/ref/star_ref -t 8 -at STAR
# make sure the directory exists - e.g. include in user data file (AWS EC2)
instance-working-dir = /mnt1/working
# fastq
# manifest file lists fastq file names (pathless)
# one per line unless pe - then tab separated
[fastq]
manifest = test_small_2_fastq.manifest
s3-bucket-addr = vccri.ho-lab.software-testing
region = us-west-2
#s3-bucket-dir = andri/nhmrc_morescdataset_fastq
s3-bucket-dir = mictro/data
destination-dir = /mnt1/data
# data to copy from S3 to local instances
# Note for naming copy-s3 sections, the section name must start with:
# "copy-s3" ; e.g. [copy-s3-whatever-1]
# The following has been included as an example
[copy-s3-small-1]
s3-bucket-addr = vccri.ho-lab.software-testing
region = us-west-2
s3-bucket-dir = reference/small
#destination-dir = /mnt2/ref/small
destination-dir = /mnt1/ref/small
files = chr11.dict
    chr11.fa
    chr11.fa.amb
    chr11.fa.ann
    chr11.fa.bwt
    chr11.fa.fai
    chr11.fa.pac
    chr11.fa.sa
# The following has been included as an example
[copy-s3-small-2]
s3-bucket-addr = vccri.ho-lab.software-testing
region = us-west-2
s3-bucket-dir = data
destination-dir = /mnt1/data
files = H801VADXX-1-701-501_TAAGGCGA_L001_10000READS_R1_001.fastq
    H801VADXX-1-701-501_TAAGGCGA_L001_10000READS_R2_001.fastq
    exact.2nd-10000.1.fastq
    exact.2nd-10000.2.fastq
    r2.10000.1.fastq
    r2.10000.2.fastq
    art.10000.1.fastq
    art.10000.2.fastq
    sim-10000-read1.fq
    sim-10000-read2.fq
    small.1.fastq
    small.2.fastq
